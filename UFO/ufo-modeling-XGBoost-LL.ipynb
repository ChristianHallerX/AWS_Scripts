{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost and Linear Learner in SageMaker\n",
    "\n",
    "## Goal:\n",
    "Model to decide (classify) if newly reported UFO sightings are legitimate (1: explained, 2: unexplained, 3: probable)\n",
    "\n",
    "## Steps:\n",
    "1. [Load csv from S3](#Step-1:-Loading-csv-from-S3)\n",
    "1. [Cleaning, transforming, analyize, and preparing dataset](#Step-2:-Cleaning,-transforming,-analyize,-and-preparing-the-dataset)\n",
    "1. [Create and train XGBoost](#Step-3:-Train-XGBoost)\n",
    "1. [Create and train Linear Learner](#Step-4:-Linear-Learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading csv from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket = 'ml_ufo_sightings'\n",
    "sub_folder = 'ufo_dataset'\n",
    "data_key = 'ufo_fullset.csv'\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, sub_folder, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location, low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning, transforming, analyize, and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# any missing values?\n",
    "#boolean\n",
    "missing_values = df.isnull().values.any()\n",
    "# if true, then show null rows\n",
    "if(missing_values):\n",
    "    display(df[df.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values are missing in the 'shape' feature. What are the other 'shape' values ordered by counts?\n",
    "df['shape'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing 'shape' values with the most common shape -> count index 0\n",
    "df['shape'] = df['shape'].fillna(df['shape'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the `reportedTimestamp` and `eventDate` to a datetime data types.\n",
    "df['reportedTimestamp'] = pd.to_datetime(df['reportedTimestamp'])\n",
    "df['eventDate'] = pd.to_datetime(df['eventDate'])\n",
    "\n",
    "# Convert the `shape` and `weather` to a category data type.\n",
    "df['shape'] = df['shape'].astype('category')\n",
    "df['weather'] = df['weather'].astype('category')\n",
    "\n",
    "# Map the `physicalEvidence` and `contact` from 'Y', 'N' to `0`, `1`.\n",
    "df['physicalEvidence'] = df['physicalEvidence'].replace({'Y': 1, 'N': 0})\n",
    "df['contact'] = df['contact'].replace({'Y': 1, 'N': 0})\n",
    "\n",
    "# Convert the `researchOutcome` to a category data type (target attribute).\n",
    "df['researchOutcome'] = df['researchOutcome'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_context(\"paper\", font_scale=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph: Was Contact Made?\n",
    "m_cts = (df['contact'].value_counts())\n",
    "m_ctsx = m_cts.index\n",
    "m_ctsy = m_cts.get_values()\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "sns.barplot(x=m_ctsx, y=m_ctsy)\n",
    "ax.set_title('UFO Sightings and Contact')\n",
    "ax.set_xlabel('Was contact made?')\n",
    "ax.set_ylabel('Number of Sightings')\n",
    "ax.set_xticklabels(['No', 'Yes'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph: Physical Evidence?\n",
    "m_cts = (df['physicalEvidence'].value_counts())\n",
    "m_ctsx = m_cts.index\n",
    "m_ctsy = m_cts.get_values()\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "sns.barplot(x=m_ctsx, y=m_ctsy)\n",
    "ax.set_title('UFO Sightings and Physical Evidence')\n",
    "ax.set_xlabel('Was there physical evidence?')\n",
    "ax.set_ylabel('Number of Sightings')\n",
    "ax.set_xticklabels(['No', 'Yes'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph: UFO shapes\n",
    "m_cts = (df['shape'].value_counts())\n",
    "m_ctsx = m_cts.index\n",
    "m_ctsy = m_cts.get_values()\n",
    "f, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "sns.barplot(x=m_ctsx, y=m_ctsy)\n",
    "ax.set_title('UFO Sightings by Shape')\n",
    "ax.set_xlabel('UFO Shape')\n",
    "ax.set_ylabel('Number of Sightings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph: Weather during Sightings\n",
    "m_cts = (df['weather'].value_counts())\n",
    "m_ctsx = m_cts.index\n",
    "m_ctsy = m_cts.get_values()\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "sns.barplot(x=m_ctsx, y=m_ctsy)\n",
    "ax.set_title('UFO Sightings by Weather')\n",
    "ax.set_xlabel('Weather')\n",
    "ax.set_ylabel('Number of Sightings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph: Research Outcome\n",
    "m_cts = (df['researchOutcome'].value_counts())\n",
    "m_ctsx = m_cts.index\n",
    "m_ctsy = m_cts.get_values()\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "sns.barplot(x=m_ctsx, y=m_ctsy)\n",
    "ax.set_title('UFO Sightings and Research Outcome')\n",
    "ax.set_xlabel('Research Outcome')\n",
    "ax.set_ylabel('Number of Sightings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Graph: Sightings by Year\n",
    "ufo_yr = df['eventDate'].dt.year  # series with the year exclusively\n",
    "\n",
    "## Set axes ##\n",
    "years_data = ufo_yr.value_counts()\n",
    "years_index = years_data.index  # x ticks\n",
    "years_values = years_data.get_values()\n",
    "\n",
    "## Create Bar Plot ##\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.xticks(rotation = 60)\n",
    "plt.title('UFO Sightings by Year')\n",
    "plt.ylabel('Number of Sightings')\n",
    "plt.xlabel('Year')\n",
    "\n",
    "years_plot = sns.barplot(x=years_index[:60],y=years_values[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlaton table\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove uninformative features:\n",
    "1. `sighting`, always 'Y' \n",
    "1. `firstName` and `lastName`, uninformative for predicting `researchOutcome`\n",
    "1. `reportedTimestamp` uninformative for predicting `researchOutcome`\n",
    "1. `eventDate` and `eventTime` are very evenly distributed. There are no discernible seasons etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['firstName', 'lastName', 'sighting', 'reportedTimestamp', 'eventDate', 'eventTime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories `weather` and `shape`\n",
    "df = pd.get_dummies(df, columns=['weather', 'shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `researchOutcome` (target) to numeric values: unexplained, explained, and probable to 0, 1, 2.\n",
    "df['researchOutcome'] = df['researchOutcome'].replace({'unexplained': 0, 'explained': 1, 'probable': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomize and Split\n",
    "Tis may be easier with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle/Randomize order\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split into training, validation, testing\n",
    "rand_split = np.random.rand(len(df))\n",
    "train_list = rand_split < 0.8                       # 80% training\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9) # 10% validation\n",
    "test_list = rand_split >= 0.9                       # 10% testing\n",
    "\n",
    "data_train = df[train_list]\n",
    "data_val = df[val_list]\n",
    "data_test = df[test_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost).\n",
    "\n",
    "After that we will go ahead and create those files on our Notebook instance (stored as CSV) and then upload them to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move target 'researchOutcome' to the first position, then create CSV files\n",
    "pd.concat([data_train['researchOutcome'], data_train.drop(['researchOutcome'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([data_val['researchOutcome'], data_val.drop(['researchOutcome'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "# upload CSV to S3 into train and validation folders\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/xgboost_train/train.csv').upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/xgboost_validation/validation.csv').upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with the CSV file format -> create inputs that training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/algorithms_lab/xgboost_train'.format(bucket), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/algorithms_lab/xgboost_validation'.format(bucket), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training job name\n",
    "job_name = 'ufo-xgboost-job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print('Here is the job name {}'.format(job_name))\n",
    "\n",
    "# model output path\n",
    "output_location = 's3://{}/algorithms_lab/xgboost_output'.format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=sess\n",
    "                                   )\n",
    "\n",
    "xgb.set_hyperparameters(objective='multi:softmax',\n",
    "                        num_class=3,\n",
    "                        num_round=100\n",
    "                       )\n",
    "\n",
    "data_channels = {'train': s3_input_train,\n",
    "                'validation': s3_input_validation\n",
    "                }\n",
    "\n",
    "# call `.fit()` function to start training\n",
    "xgb.fit(data_channels, job_name=job_name)\n",
    "\n",
    "print('Model name: {}/{}/output/model.tar.gz'.format(output_location, job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, see the default evaluation metric in the logs. Also access detailed logs in CloudWatch.\n",
    "\n",
    "The `merror` is used in multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases). This needs to be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Linear Learner\n",
    "\n",
    "Randomize the data again and get it ready for the Linear Leaner algorithm. \n",
    "\n",
    "Rearrange the columns so it is ready for the algorithm (first column = target attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "rand_split = np.random.rand(len(df))\n",
    "train_list = rand_split < 0.8\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9)\n",
    "test_list = rand_split >= 0.9\n",
    "\n",
    "data_train = df[train_list]\n",
    "data_val = df[val_list]\n",
    "data_test = df[test_list]\n",
    "\n",
    "\n",
    "\n",
    "# Put label up first for all three DFs\n",
    "cols = list(data_train)\n",
    "cols.insert(0, cols.pop(cols.index('researchOutcome')))\n",
    "data_train = data_train[cols]\n",
    "\n",
    "cols = list(data_val)\n",
    "cols.insert(0, cols.pop(cols.index('researchOutcome')))\n",
    "data_val = data_val[cols]\n",
    "\n",
    "cols = list(data_test)\n",
    "cols.insert(0, cols.pop(cols.index('researchOutcome')))\n",
    "data_test = data_test[cols]\n",
    "\n",
    "\n",
    "\n",
    "# Breaks the datasets into attribute numpy.ndarray and the same for target attribute.  \n",
    "train_X = data_train.drop(columns='researchOutcome').values\n",
    "train_y = data_train['researchOutcome'].values\n",
    "\n",
    "val_X = data_val.drop(columns='researchOutcome').values\n",
    "val_y = data_val['researchOutcome'].values\n",
    "\n",
    "test_X = data_test.drop(columns='researchOutcome').values\n",
    "test_y = data_test['researchOutcome'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipe Mode (Training Set): Create recordIO_protobuf file and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'ufo_sightings_train_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/linearlearner_train/{}'.format(train_file)).upload_fileobj(f)\n",
    "training_recordIO_protobuf_location = 's3://{}/algorithms_lab/linearlearner_train/{}'.format(bucket, train_file)\n",
    "\n",
    "print('The Pipe mode recordIO protobuf training data: {}'.format(training_recordIO_protobuf_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipe Mode (Validation Set): Create recordIO_protobuf file and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_file = 'ufo_sightings_validatioin_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, val_X.astype('float32'), val_y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/linearlearner_validation/{}'.format(validation_file)).upload_fileobj(f)\n",
    "validate_recordIO_protobuf_location = 's3://{}/algorithms_lab/linearlearner_validation/{}'.format(bucket, validation_file)\n",
    "\n",
    "print('The Pipe mode recordIO protobuf validation data: {}'.format(validate_recordIO_protobuf_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Docker image with Linear Learner Algorithm from ECR repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner', \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training job name\n",
    "job_name = 'ufo-linear-learner-job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print('Here is the job name {}'.format(job_name))\n",
    "\n",
    "# model-artifact oupput path\n",
    "output_location = 's3://{}/algorithms_lab/linearlearner_output'.format(bucket)\n",
    "\n",
    "print('Feature_dim hyperparameter needs to be set to {}.'.format(data_train.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "# Setup the LinearLeaner algorithm from the ECR container\n",
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sess,\n",
    "                                       input_mode='Pipe'\n",
    "                                      )\n",
    "\n",
    "# Setup the hyperparameters\n",
    "linear.set_hyperparameters(feature_dim=22, # number of attributes (minus the researchOutcome attribute)\n",
    "                           predictor_type='multiclass_classifier', # type of classification problem\n",
    "                           num_classes=3 # number of classes in out researchOutcome (explained, unexplained, probable)\n",
    "                            )  \n",
    "\n",
    "\n",
    "# data input\n",
    "data_channels = {\n",
    "                'train': training_recordIO_protobuf_location,\n",
    "                'validation': validate_recordIO_protobuf_location\n",
    "                }\n",
    "\n",
    "# start fit job\n",
    "linear.fit(data_channels, job_name=job_name)\n",
    "\n",
    "print('Output path of Linear Learner model: {}/{}/output/model.tar.gz'.format(output_location, job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Go to CloudWatch to see detailed model logs.\n",
    "- Stop the Notebook Instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
